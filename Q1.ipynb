{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prakhar Jain\n",
    "# 2022121008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "from preprocess import TextProcessor, Vocabulary, TextDataset, TextDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement the Language Model and report the Perplexity Scores. [40 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Auguste_Maquet.txt\"\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "# can use fasttext as well just by specifying the path\n",
    "\n",
    "text_processor = TextProcessor(file_path)\n",
    "sentences = text_processor.preprocess_text()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "val_len = int(len(sentences) * 0.1)\n",
    "test_len = int(len(sentences) * 0.2)\n",
    "\n",
    "train_sentences = sentences[val_len + test_len:]\n",
    "val_sentences = sentences[:val_len]\n",
    "test_sentences = sentences[val_len:val_len + test_len]\n",
    "\n",
    "train_ngrams = text_processor.generate_ngrams(train_sentences, 5 + 1) # 5 for context and 1 for target\n",
    "val_ngrams = text_processor.generate_ngrams(val_sentences, 5 +1)\n",
    "test_ngrams = text_processor.generate_ngrams(test_sentences, 5 + 1)\n",
    "\n",
    "vocabulary = Vocabulary(glove_path)\n",
    "vocabulary.build_vocab(train_sentences)\n",
    "embeddings = vocabulary.get_glove_embeddings()\n",
    "\n",
    "train_dataset = TextDataset(train_ngrams, vocabulary)\n",
    "val_dataset = TextDataset(val_ngrams, vocabulary)\n",
    "test_dataset = TextDataset(test_ngrams, vocabulary)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 34840\n",
      "Train: 24388\n",
      "Val: 3484\n",
      "Test: 6968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentences: {len(sentences)}\")\n",
    "\n",
    "print(f\"Train: {len(train_sentences)}\")\n",
    "print(f\"Val: {len(val_sentences)}\")\n",
    "print(f\"Test: {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> copyright laws in most countries\n",
      "<S>\n",
      "1 1\n",
      "copyright\n",
      "3 3\n",
      "laws\n",
      "4 4\n",
      "in\n",
      "5 5\n",
      "most\n",
      "6 6\n",
      "countries\n",
      "7 7\n"
     ]
    }
   ],
   "source": [
    "## test functionality\n",
    "print(train_ngrams[0])\n",
    "\n",
    "for batch in train_loader:\n",
    "    for x in batch[0][0]:\n",
    "        print(vocabulary.index2word(x.item()))\n",
    "        print(str(x.item()) +\" \"+str(vocabulary.word2index(vocabulary.index2word(x.item()))))\n",
    "\n",
    "    # print the target word\n",
    "    print(vocabulary.index2word(batch[1][0].item()))\n",
    "    print(str(batch[1][0].item()) +\" \"+str(vocabulary.word2index(vocabulary.index2word(batch[1][0].item()))))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18843, 300])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding, dropout):\n",
    "        super(NeuralLanguageModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(5 * embedding_dim, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(300, vocab_size)\n",
    "            # nn.LogSoftmax(dim=1) # CrossEntropyLoss already applies log softmax\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (batch_size, 5)    \n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(-1, 5 * self.embedding_dim) # flatten the tensor\n",
    "        output = self.model(embeds)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "Check out reduction parameter in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLanguageModel(vocab_size=embeddings.shape[0], embedding_dim=embeddings.shape[1], hidden_dim=300, embedding=embeddings, dropout=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d770e6de714c70bbb7cf3e2623447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea39b3087674ee58a0c614630f6c7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Loss: 5.9379 Train Perplexity: 379.1531 Val Loss: 5.4780 Val Perplexity: 239.3634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b6b0aa8013468f88112775c240e555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8472a4a4c1449a49c503b06ee08387a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Train Loss: 5.5988 Train Perplexity: 270.0974 Val Loss: 5.3377 Val Perplexity: 208.0315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc14fb54af14908937f116216376296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6464c422f90743bfae237b85ca0394cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Train Loss: 5.5187 Train Perplexity: 249.3163 Val Loss: 5.3027 Val Perplexity: 200.8800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a365e1d06436c81e4041a3b5a4281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a1836bbed94393997ec63a38dc9df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Train Loss: 5.4741 Train Perplexity: 238.4338 Val Loss: 5.2610 Val Perplexity: 192.6802\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccfe75a50bf4f2ab4a8f571eaef9522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02ade787af24afdb92bd823064c7d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Train Loss: 5.4496 Train Perplexity: 232.6667 Val Loss: 5.2837 Val Perplexity: 197.1030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b3a16bb8d84706859f756f7ded53f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b98dc1890346c5b45e76bb19b83ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Train Loss: 5.4233 Train Perplexity: 226.6206 Val Loss: 5.2608 Val Perplexity: 192.6284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b817f9be5d4659826fdf84871693c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f0140e1f7f43ca89d4181f82dcbcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Train Loss: 5.4052 Train Perplexity: 222.5682 Val Loss: 5.2773 Val Perplexity: 195.8392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279a1ecb7682432ca51a7873fed44cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb373282eed545d993a87f182a1a55e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Train Loss: 5.3913 Train Perplexity: 219.4792 Val Loss: 5.2571 Val Perplexity: 191.9179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a4365093a41af8c3d5544b497f9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f14df327dbe462d85ad5e8abb28cc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Train Loss: 5.3805 Train Perplexity: 217.1264 Val Loss: 5.2537 Val Perplexity: 191.2733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6ad89fa9ff4fcc898c11203b4770b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5927353c8141fe8bedc6f38ed626ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Train Loss: 5.3761 Train Perplexity: 216.1799 Val Loss: 5.2571 Val Perplexity: 191.9220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5931fc3d3ea4bbaab7c73642f731f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.2591 Test Perplexity: 192.3162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_perplexity(loss):\n",
    "    # perplexity = exp(cross_entropy) this is a direct relationship between cross entropy and perplexity\n",
    "    return torch.exp(torch.tensor(loss))\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        context, target = batch\n",
    "        context, target = context.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    # avg_loss = total_loss / len(train_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(val_loader):\n",
    "            context, target = batch\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    # avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_loader):\n",
    "            context, target = batch\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    # avg_loss = total_loss / len(test_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_perplexity = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} Train Loss: {train_loss:.4f} Train Perplexity: {train_perplexity:.4f} Val Loss: {val_loss:.4f} Val Perplexity: {val_perplexity:.4f}\")\n",
    "\n",
    "\n",
    "test_loss, test_perplexity = test(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f} Test Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Bonus** Plot graphs showing the variation of average train/test perplexities with varying hyperparameters  like Dropout rate, changing the dimensions of the layers, changing the Optimizer, etc. Report the most optimal Hyperparameters found. [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (686894790.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    'hidden_dim': ,\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "HYPERPARAMS = {\n",
    "    'hidden_dim': ,\n",
    "    'optimizer': ,\n",
    "    'dropout': ,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
