{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, file_path) -> None:\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^0-9a-zA-Z?.,!:;]+\", r\" \", text)\n",
    "        text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self):\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            corpus = f.read()\n",
    "        sentences = sent_tokenize(corpus)\n",
    "        sentences = [self.clean_text(sent) for sent in sentences if not sent.lower(\n",
    "        ).startswith('chapter') and not sent[0].isdigit()]\n",
    "        return sentences\n",
    "\n",
    "    def get_max_len(self, sentences):\n",
    "        return max([len(word_tokenize(sent)) for sent in sentences])\n",
    "\n",
    "    def generate_sentences(self, sentences):\n",
    "        padded_sentences = []\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            padded_sentence = ['<S>'] + tokens + ['</S>']\n",
    "            padded_sentence = ' '.join(padded_sentence)\n",
    "            padded_sentences.append(padded_sentence)\n",
    "        return padded_sentences\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, glove_path):\n",
    "        self.glove_model = KeyedVectors.load_word2vec_format(\n",
    "            glove_path, binary=False, no_header=True)\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<S>', '</S>']\n",
    "        self._add_special_tokens()\n",
    "\n",
    "    def _add_special_tokens(self):\n",
    "        for token in self.special_tokens:\n",
    "            self.add_word(token)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                self.add_word(token)\n",
    "\n",
    "    def get_glove_embeddings(self):\n",
    "        embedding_dim = self.glove_model.vector_size\n",
    "        embeddings = np.zeros((len(self.idx2word), embedding_dim))\n",
    "        for idx, word in enumerate(self.idx2word):\n",
    "            if word in self.glove_model:\n",
    "                embeddings[idx] = self.glove_model[word]\n",
    "            else:\n",
    "                embeddings[idx] = np.random.normal(\n",
    "                    scale=0.6, size=(embedding_dim,))\n",
    "        return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "    def index2word(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def word2index(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<UNK>'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class TextDatasetTransformer(Dataset):\n",
    "    def __init__(self, sentences, vocab, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx].split()\n",
    "        if len(sentence) > self.max_len:\n",
    "            sentence = sentence[:self.max_len]\n",
    "        input_idxs = [self.vocab.word2index(word) for word in sentence[:-1]]\n",
    "        target_idxs = [self.vocab.word2index(word) for word in sentence[1:]]\n",
    "\n",
    "        return torch.tensor(input_idxs, dtype=torch.long), torch.tensor(target_idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch, padding_value):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    padded_inputs = pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=padding_value)\n",
    "    padded_targets = pad_sequence(\n",
    "        targets, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return padded_inputs, padded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Auguste_Maquet.txt\"\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "text_processor = TextProcessor(file_path)\n",
    "sentences = text_processor.preprocess_text()\n",
    "\n",
    "max_len = text_processor.get_max_len(sentences)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "val_len = int(len(sentences) * 0.1)\n",
    "test_len = int(len(sentences) * 0.2)\n",
    "\n",
    "train_sentences = sentences[val_len + test_len:]\n",
    "val_sentences = sentences[:val_len]\n",
    "test_sentences = sentences[val_len:val_len + test_len]\n",
    "\n",
    "vocabulary = Vocabulary(glove_path)\n",
    "vocabulary.build_vocab(train_sentences)\n",
    "embeddings = vocabulary.get_glove_embeddings()\n",
    "\n",
    "train_sentences = text_processor.generate_sentences(train_sentences)\n",
    "val_sentences = text_processor.generate_sentences(val_sentences)\n",
    "test_sentences = text_processor.generate_sentences(test_sentences)\n",
    "\n",
    "train_dataset = TextDatasetTransformer(train_sentences, vocabulary, max_len)\n",
    "val_dataset = TextDatasetTransformer(val_sentences, vocabulary, max_len)\n",
    "test_dataset = TextDatasetTransformer(test_sentences, vocabulary, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padding_value = vocabulary.word2index('<PAD>')\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         collate_fn=lambda batch: collate_fn(batch, padding_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, dropout, max_len):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=True)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # def generate_square_subsequent_masks(self, sz):\n",
    "    #     return nn.Transformer.generate_square_subsequent_mask(sz)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(inputs.size(1)).to(inputs.device)\n",
    "        input_embedded = self.embedding(inputs) + self.pos_embedding(torch.arange(inputs.size(1)).unsqueeze(0).to(inputs.device))\n",
    "        output = self.decoder(input_embedded, memory = input_embedded , tgt_mask=tgt_mask, memory_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embeddings.shape[0]\n",
    "embedding_dim = embeddings.shape[1]\n",
    "hidden_dim = 512\n",
    "num_layers = 6\n",
    "num_heads = 6\n",
    "dropout = 0.1\n",
    "max_seq_len = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoder(vocab_size, embedding_dim, hidden_dim,\n",
    "                           num_layers, num_heads, dropout, max_seq_len).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_value, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_square_subsequent_mask(sz):\n",
    "#     \"\"\"\n",
    "#     Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "#     Unmasked positions are filled with float(0.0).\n",
    "#     \"\"\"\n",
    "#     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for inputs, targets in tqdm(dataloader, desc='Training...'):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += (targets != padding_value).sum().item()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        for inputs, targets in tqdm(dataloader, desc='Evaluating...'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (targets != padding_value).sum().item()\n",
    "    # avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        for inputs, targets in tqdm(dataloader, desc='Testing...'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (targets != padding_value).sum().item()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba5650443854d94b0be7a1f20f3d9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/3049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d5af05310545b4b5294147b068cb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 => Train Loss: 6.1477, Train Perplexity: 467.6409, Val Loss: 4173.4170, Val Perplexity: inf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa76f605d70a4515ae44abe53acf529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/3049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 4\u001b[0m     train_loss, train_perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     val_loss, val_perplexity \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_perplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 15\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     total_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m!=\u001b[39m padding_value)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m total_tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train_loss, train_perplexity = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {i+1}/{n_epochs} => Train Loss: {train_loss:.4f}, Train Perplexity: {train_perplexity:.4f}, Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "test_loss, test_perplexity = test(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
