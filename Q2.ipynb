{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prakhar Jain\n",
    "# 2022121008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained Models : https://drive.google.com/drive/folders/1E1pYtju_sBlSnodKjQa1b_uAq80xNbYN?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prakhar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/prakhar/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "# import nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# from preprocess import TextProcessor, Vocabulary, TextDatasetLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, file_path) -> None:\n",
    "        self.file_path = file_path\n",
    "# you can also try stemming and lemmatizers to improve performance\n",
    "# https://towardsdatascience.com/text-preprocessing-with-nltk-9de5de891658\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^0-9a-zA-Z?.,!:;]+\", r\" \", text)\n",
    "        text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self):\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            corpus = f.read()\n",
    "        sentences = sent_tokenize(corpus)\n",
    "        sentences = [self.clean_text(sent) for sent in sentences if not sent.lower(\n",
    "        ).startswith('chapter') and not sent[0].isdigit()]\n",
    "        return sentences\n",
    "\n",
    "    # def generate_ngrams(self, sentences, n):\n",
    "    #     ngrams = []\n",
    "    #     for sentence in sentences:\n",
    "    #         tokens = word_tokenize(sentence)\n",
    "    #         tokens = ['<S>'] + tokens + ['</S>']\n",
    "    #         # <s> helps in identifying the start of a sentence, allowing the model to learn the patterns and probabilities associated with words that commonly appear at the beginning of sentences\n",
    "    #         # <\\s> helps the model understand when the a sentence is complete and learn word sentences that commonly end sentences.\n",
    "    #         sentence_ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    #         ngrams.extend([' '.join(ngram) for ngram in sentence_ngrams])\n",
    "    #     return ngrams\n",
    "\n",
    "    def get_max_len(self, sentences):\n",
    "        return max([len(word_tokenize(sent)) for sent in sentences])\n",
    "\n",
    "    # def get_max_len(self, sentences):\n",
    "    #     max_len = 0\n",
    "    #     max_len_sentence = \"\"\n",
    "    #     for sent in sentences:\n",
    "    #         tokenized_sent = word_tokenize(sent)\n",
    "    #         if len(tokenized_sent) > max_len:\n",
    "    #             max_len = len(tokenized_sent)\n",
    "    #             max_len_sentence = sent\n",
    "    #     return max_len, max_len_sentence\n",
    "\n",
    "    def generate_lstm_sentences(self, sentences):\n",
    "        padded_sentences = []\n",
    "        # get the maximum length of the sentence\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            padded_sentence = tokens\n",
    "            padded_sentence = ['<S>'] + tokens + ['</S>']\n",
    "            padded_sentence = ' '.join(padded_sentence)\n",
    "            padded_sentences.append(padded_sentence)\n",
    "        return padded_sentences\n",
    "\n",
    "    # def generate_padded_sentences(self, sentences, max_len):\n",
    "    #     padded_sentences = []\n",
    "    #     # get the maximum length of the sentence\n",
    "    #     for sentence in sentences:\n",
    "    #         tokens = word_tokenize(sentence)\n",
    "    #         padded_sentence = tokens\n",
    "    #         padded_sentence = ['<S>'] + tokens + ['</S>']\n",
    "    #         padded_sentence += ['<PAD>'] * (max_len - len(tokens))\n",
    "    #         padded_sentence = ' '.join(padded_sentence)\n",
    "    #         padded_sentences.append(padded_sentence)\n",
    "    #     return padded_sentences\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, glove_path):\n",
    "        self.glove_model = KeyedVectors.load_word2vec_format(\n",
    "            glove_path, binary=False, no_header=True)\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<S>', '</S>']\n",
    "        self._add_special_tokens()\n",
    "\n",
    "    def _add_special_tokens(self):\n",
    "        for token in self.special_tokens:\n",
    "            self.add_word(token)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                self.add_word(token)\n",
    "\n",
    "    def get_glove_embeddings(self):\n",
    "        embedding_dim = self.glove_model.vector_size\n",
    "        embeddings = np.zeros((len(self.idx2word), embedding_dim))\n",
    "        # embeddings in the shape of (vocab_size, embedding_dim)\n",
    "        for idx, word in enumerate(self.idx2word):\n",
    "            if word in self.glove_model:\n",
    "                embeddings[idx] = self.glove_model[word]\n",
    "            else:\n",
    "                embeddings[idx] = np.random.normal(\n",
    "                    scale=0.6, size=(embedding_dim,))\n",
    "        return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "    def index2word(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def word2index(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx['<UNK>'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class TextDatasetLSTM(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx].split()\n",
    "        input_idxs = [self.vocab.word2index(word) for word in sentence[:-1]]  # All words except the last one\n",
    "        target_idxs = [self.vocab.word2index(word) for word in sentence[1:]]  # All words except the first one\n",
    "        return torch.tensor(input_idxs, dtype=torch.long), torch.tensor(target_idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch, padding_value):\n",
    "    # Sort batch by sequence length in descending order\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(seq) for seq in inputs]  # Extract original sequence lengths\n",
    "    \n",
    "    # Pad sequences to the max length in the batch\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=padding_value)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    return padded_inputs, padded_targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Auguste_Maquet.txt\"\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "# can use fasttext as well just by specifying the path\n",
    "\n",
    "text_processor = TextProcessor(file_path)\n",
    "sentences = text_processor.preprocess_text()\n",
    "\n",
    "max_len = text_processor.get_max_len(sentences)\n",
    "\n",
    "# print(max_len)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "val_len = int(len(sentences) * 0.1)\n",
    "test_len = int(len(sentences) * 0.2)\n",
    "\n",
    "train_sentences = sentences[val_len + test_len:]\n",
    "val_sentences = sentences[:val_len]\n",
    "test_sentences = sentences[val_len:val_len + test_len]\n",
    "\n",
    "vocabulary = Vocabulary(glove_path)\n",
    "vocabulary.build_vocab(train_sentences)\n",
    "embeddings = vocabulary.get_glove_embeddings()\n",
    "\n",
    "train_sentences = text_processor.generate_lstm_sentences(train_sentences)\n",
    "val_sentences = text_processor.generate_lstm_sentences(val_sentences)\n",
    "test_sentences = text_processor.generate_lstm_sentences(test_sentences)\n",
    "\n",
    "train_dataset = TextDatasetLSTM(train_sentences, vocabulary)\n",
    "val_dataset = TextDatasetLSTM(val_sentences, vocabulary)\n",
    "test_dataset = TextDatasetLSTM(test_sentences, vocabulary)\n",
    "\n",
    "padding_value = vocabulary.word2index('<PAD>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         collate_fn=lambda batch: collate_fn(batch, padding_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     # print(batch)\n",
    "#     print(batch.shape)\n",
    "#     print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, padding_value, embeddings):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings, padding_idx=padding_value, freeze=True)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, _ = self.lstm(packed_embedded)\n",
    "\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_input, batch_target, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch_input, lengths)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, output.size(-1)), batch_target.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target, lengths in val_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_input, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, output.size(-1)), batch_target.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target, lengths in test_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_input, lengths)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.view(-1, output.size(-1)), batch_target.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(torch.tensor(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embeddings.shape[0]\n",
    "embedding_dim = embeddings.shape[1]\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "output_dim = embeddings.shape[0]\n",
    "padding_value = vocabulary.word2index('<PAD>')\n",
    "\n",
    "model = LSTM(vocab_size=vocab_size, \n",
    "             embedding_dim=300, \n",
    "             hidden_dim=512, \n",
    "             num_layers=2, \n",
    "             output_dim=vocab_size,  # same as vocab_size\n",
    "             padding_value=padding_value, \n",
    "             embeddings=embeddings).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_value)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 5.8777, Train Perplexity: 356.99, Val Loss: 5.2042, Val Perplexity: 182.03\n",
      "Epoch: 2, Train Loss: 4.9595, Train Perplexity: 142.53, Val Loss: 4.8455, Val Perplexity: 127.17\n",
      "Epoch: 3, Train Loss: 4.6837, Train Perplexity: 108.17, Val Loss: 4.6918, Val Perplexity: 109.04\n",
      "Epoch: 4, Train Loss: 4.5109, Train Perplexity: 91.00, Val Loss: 4.5819, Val Perplexity: 97.70\n",
      "Epoch: 5, Train Loss: 4.3814, Train Perplexity: 79.95, Val Loss: 4.5218, Val Perplexity: 92.00\n",
      "Epoch: 6, Train Loss: 4.2769, Train Perplexity: 72.02, Val Loss: 4.4757, Val Perplexity: 87.86\n",
      "Epoch: 7, Train Loss: 4.1904, Train Perplexity: 66.05, Val Loss: 4.4441, Val Perplexity: 85.12\n",
      "Epoch: 8, Train Loss: 4.1122, Train Perplexity: 61.08, Val Loss: 4.4060, Val Perplexity: 81.94\n",
      "Epoch: 9, Train Loss: 4.0459, Train Perplexity: 57.16, Val Loss: 4.3802, Val Perplexity: 79.86\n",
      "Epoch: 10, Train Loss: 3.9886, Train Perplexity: 53.98, Val Loss: 4.3656, Val Perplexity: 78.70\n",
      "Test Loss: 4.3604, Test Perplexity: 78.29\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_perplexity = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_perplexity = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Perplexity: {train_perplexity:.2f}, Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.2f}\")\n",
    "\n",
    "test_loss, test_perplexity = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embeddings.shape[0]\n",
    "padding_value = vocabulary.word2index('<PAD>')\n",
    "\n",
    "model = LSTM(vocab_size=vocab_size, \n",
    "             embedding_dim=300, \n",
    "             hidden_dim=512, \n",
    "             num_layers=2, \n",
    "             output_dim=vocab_size,  # same as vocab_size\n",
    "             padding_value=padding_value, \n",
    "             embeddings=embeddings).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         collate_fn=lambda batch: collate_fn(batch, padding_value))\n",
    "\n",
    "# write perplexity for each sentence in train_loader, val_loader and test_loader in a file LM-2-training.txt, LM-2-validation.txt and LM-2-testing.txt respectively\n",
    "# write the perplexity of each sentence in a new line\n",
    "def write_perplexity(model, data_loader, criterion, device, file_path):\n",
    "    model.eval()\n",
    "    with open(file_path, 'w') as f:\n",
    "        for batch_input, batch_target, lengths in data_loader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            \n",
    "\n",
    "            output = model(batch_input, lengths)\n",
    "            \n",
    "            \n",
    "            loss = criterion(output.view(-1, output.size(-1)), batch_target.view(-1))\n",
    "            perplexity = calculate_perplexity(loss.item())\n",
    "\n",
    "            #  convert batch_input to sentence\n",
    "            sentence = ' '.join([vocabulary.index2word(idx.item()) for idx in batch_input[0]])\n",
    "            f.write(f\"{sentence}: {perplexity:.2f}\\n\")\n",
    "    \n",
    "write_perplexity(model, train_loader, criterion, device, 'LM-2-training.txt')\n",
    "write_perplexity(model, val_loader, criterion, device, 'LM-2-validation.txt')\n",
    "write_perplexity(model, test_loader, criterion, device, 'LM-2-testing.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
